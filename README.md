# Internal_Comp_Fall2019
## Max Wenzel and Michael Gigiolio Project


### Summary


To begin the semester, we investigated potential paths we could take for the final project. Our initial thought was to find some way to implement a GANN, or a generated, adversarial neural network in order to not only perform some type of classification, but also to have a working output with which we could analyze the progress of our creation. As our team holds some degree of experience with natural language processing, we decided that incorporating some degree of NLP with our GANN could yield interesting results. We settled on a very classic representation of text for our processing, the works of William Shakespeare. The majority of English speakers are able to differentiate between Shakespearean English and modern English and, as a result, making a network that could distinguish between them as well would be a very interesting task. Furthermore, by creating a system that was also to generate vaguely Shakespearean text, we could also evaluate the user’s ability to recognize that form of English as well. In addition, this output would provide useful feedback on the quality of the classification system itself. 


To initialize this system, we decided that we would begin by categorizing twitter posts by both President Barack Obama as well as President Donald Trump. This is because there is a very notable difference between the two of their writing styles and the content is much shorter and more manageable. This would provide us with a general understanding of how well our system works before moving onto the next portion. However, part way through our creation of this GANN and after we had collected and organized all the data, we made the executive decision to instead work on the internal competition. Fortunately, our work was still in fairly rudimentary states and, resultingly, not too much work was lost. From here, we dedicated our work to the internal competition and the very different form of machine learning needed to properly execute it.


In approaching the internal competition, we first started by looking at the problem and seeing what the best way to solve it could be. We, of course, initially considered simply performing a k-nearest neighbors analysis, but scrapped this due to the obvious constraints of being required to work in 600 dimensional space. Though very cool and scifi sounding, this is wildly impractical and would take far more time than desired and, potentially, even then had. Instead, we turned our attention to recurrent neural networks. We elected on this format due to the fact that the data we received is in the form of a time series and recurrent neural networks are, among other things, specialized to handle this manner of data.


To begin our handling of this data, we first looked through the data itself. It was possible that physically graphing the data could yield some interesting insights on how to proceed. However, when graphed, all the data spans a very large range of values with no wells being particularly noteworthy in the grand scheme of things. We then considered the possibility that our inability to see any trends resulted from the data being in an unnormalized form. By determining the mean and standard deviation of each row, subtracting the mean from each data point and then dividing it by the standard deviation, we were able to get data with a mean of around 0 and much more easy to compare. However, when these were all graphed together, the clump appeared like a brown line of toothpaste on the graph, showing us that, visually, analysis of the graph would be fruitless. Thusly, we decided to progress with our LSTM recurrent neural network. 


In order to set up our data for the LSTM, we converted our data into a one hot system, thereby making it far easier to handle. From this we fed it into the KERAS LSTM system and then gave those outputs to a softmax. Initially, we got terrible results, roughly about ⅓ accuracy for a system with 3 categories, but we discovered that this was owing to our usage of the raw data rather than the normalized data. Because of this, we normalized the data and then performed analysis. We noted significantly better, but still not good results after doing this, having an average of about 48% accuracy. Noting the progress but lack of success, we elected to instead make our LSTM bidirectional. This yielded significantly better results, reaching the end accuracy of the previous versions in ⅜ epochs. Having discovered the drastic increase in performance from this, we halted the analysis and added yet another layer to our neural network to see if we could further improve it through that. 


After running our LSTM through eight epochs using two layers and bidirectionality, we were able to get a training accuracy of approximately 49%. This is not significantly higher than previous models, but this can mainly be attributed to a relatively low amount of epochs and laters. Though this could be increased, in its current state the code takes over an hour to run and, as a result, increasing the rigorousness of the code would be a massive time detriment. Regardless, these are still relatively good results and show that the code we constructed does, in fact, work, it simply needs further refinement.


Once all our results were recorded in a file, we needed to process the output in some way. As the output was simply a long list of three numbers, all with differing values, we required some way to get a more appetizing dataset. Our process to do this is simple. We simply add up all the values to determine the most common result for each well. Though another way to do this would be to quantify how many of each well win each time, by adding them all up, greater degrees of confidence are taken into account more than narrow victories, making sure that good results are better than many results.


Several very important things stood out as lessons from this project. Firstly, data normalization is incredibly important and, though frequently time consuming and complicated, it is almost always worth it in the long run. Furthermore, if the data is not already normalized, doing so could be a potential way to improve functioning. Secondly, when dealing with large datasets of relatively similar data, visualization is often a waste of time. Though it is generally not too difficult to do, important information should not be expected from visual analysis and it may be advisable to simply move on to more advanced methods.


Our goals for this project were, fundamentally, to create a system which could, to some degree of accuracy, categorize the data by wells. Though our results are not as good as we may have liked them to be, considering the massive time constraints imposed on us by other classes as well as other factors that impeded our progress, we did quite well, and this has been a significant learning experience for the both of us that will greatly aid us in our future computer science endeavours. 

